{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import UniSkip, Encoder\n",
    "from data_loader import DataLoader\n",
    "from vocab import load_dictionary\n",
    "from config import *\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from numpy import NaN\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.stats import spearmanr, pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_Regressor(X, y):\n",
    "    #kfold\n",
    "    kf = model_selection.KFold(n_splits=5) \n",
    "    kf.get_n_splits(X)\n",
    "    #print(kf)\n",
    "    #creating instance of RFRegressor \n",
    "    model1 = RandomForestRegressor(n_estimators=500,max_features='sqrt',n_jobs=-1,min_samples_leaf=60)#cross val split\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #print('TRAIN:', train_index, 'TEST:', test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]    \n",
    "    #training\n",
    "    model1.fit(X_train, y_train)\n",
    "    #evaluating\n",
    "    y_pred = model1.predict(X_test)\n",
    "    #monotonic relationship as the relation between the variables is non linear\n",
    "    spearman = spearmanr(y_test, y_pred)\n",
    "    pearson = pearsonr(y_test, y_pred)\n",
    "    print(f'Test data Spearman correlation: {spearman[0]:.3}')\n",
    "    print(f'Test data pearson correlation: {pearson[0]:.3}')\n",
    "    print('MSE')\n",
    "    print(metrics.mean_squared_error(y_test, y_pred)) #mean square error\n",
    "    print('R2')\n",
    "    print(metrics.r2_score(y_test, y_pred)) #r2 score\n",
    "    print('MAE')\n",
    "    print(metrics.mean_absolute_error(y_test, y_pred)) #mae\n",
    "    print('Variance Score')\n",
    "    print(metrics.explained_variance_score(y_test, y_pred)) #mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, y):\n",
    "    #kfold\n",
    "    kf = model_selection.KFold(n_splits=5) \n",
    "    kf.get_n_splits(X)\n",
    "    #print(kf)\n",
    "    #cross val split with score and r2\n",
    "    for train_index, test_index in kf.split(X):\n",
    "    #print('TRAIN:', train_index, 'TEST:', test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model2 = Ridge(alpha=0.5)\n",
    "    # Fit the model\n",
    "    model2.fit(X_train, y_train)\n",
    "    y_pred = model2.predict(X_test)\n",
    "    \n",
    "    spearman = spearmanr(y_test, y_pred)\n",
    "    pearson = pearsonr(y_test, y_pred)\n",
    "    print(f'Test data Spearman correlation: {spearman[0]:.3}')\n",
    "    print(f'Test data pearson correlation: {pearson[0]:.3}')\n",
    "    print('MSE')\n",
    "    print(metrics.mean_squared_error(y_test, y_pred)) #mean square error\n",
    "    print('R2')\n",
    "    print(metrics.r2_score(y_test, y_pred)) #r2 score\n",
    "    print('MAE')\n",
    "    print(metrics.mean_absolute_error(y_test, y_pred)) #mae\n",
    "    print('Variance Score')\n",
    "    print(metrics.explained_variance_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsableEncoder:\n",
    "\n",
    "    def __init__(self, loc=\"./saved_models/skip-best\"):\n",
    "        print(\"Preparing the DataLoader. Loading the word dictionary\")\n",
    "        self.d = DataLoader(sentences=[''], word_dict=load_dictionary('./data/new_data.txt.pkl'))\n",
    "        self.encoder = None\n",
    "\n",
    "        print(\"Loading encoder from the saved model at {}\".format(loc))\n",
    "        model = UniSkip()\n",
    "        model.load_state_dict(torch.load(loc, map_location=lambda storage, loc: storage))\n",
    "        self.encoder = model.encoder\n",
    "        if USE_CUDA:\n",
    "            self.encoder.cuda(CUDA_DEVICE)\n",
    "\n",
    "    def encode(self, text):\n",
    "        def chunks(l, n):\n",
    "            \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "            for i in range(0, len(l), n):\n",
    "                yield l[i:i + n]\n",
    "\n",
    "        ret = []\n",
    "\n",
    "        for chunk in chunks(text, 100):\n",
    "            print(\"encoding chunk of size {}\".format(len(chunk)))\n",
    "            indices = [self.d.convert_sentence_to_indices(sentence) for sentence in chunk]\n",
    "            indices = torch.stack(indices)\n",
    "            indices, _ = self.encoder(indices)\n",
    "            indices = indices.view(-1, self.encoder.thought_size)\n",
    "            indices = indices.data.cpu().numpy()\n",
    "\n",
    "            ret.extend(indices)\n",
    "        ret = np.array(ret)\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the DataLoader. Loading the word dictionary\n",
      "Making reverse dictionary\n",
      "Loading encoder from the saved model at ./saved_models/skip-best\n"
     ]
    }
   ],
   "source": [
    "t_data = UsableEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.read_csv('/Users/rithika/Documents/247ai/data/Serenata_x.csv', names=['Copy'], header=None)\n",
    "cleaned_data = cleaned_data['Copy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 100\n",
      "encoding chunk of size 18\n"
     ]
    }
   ],
   "source": [
    "X = t_data.encode(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('/Users/rithika/Documents/247ai/data/Serenata_y.csv', names=['Clicks'], header=None)\n",
    "result = result['Clicks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Spearman correlation: -0.0366\n",
      "Test data pearson correlation: 0.0275\n",
      "MSE\n",
      "187.27770068995352\n",
      "R2\n",
      "-0.06346167286077575\n",
      "MAE\n",
      "5.644443405533249\n",
      "Variance Score\n",
      "-0.0014169070514087334\n"
     ]
    }
   ],
   "source": [
    "RF_Regressor(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Spearman correlation: 0.152\n",
      "Test data pearson correlation: 0.00301\n",
      "MSE\n",
      "342.6185796682807\n",
      "R2\n",
      "-0.9455692084260989\n",
      "MAE\n",
      "12.130887224986036\n",
      "Variance Score\n",
      "-0.8589084411961652\n"
     ]
    }
   ],
   "source": [
    "ridge_regression(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
