{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import Imputer \n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from sklearn.externals import joblib\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "from bokeh.io import push_notebook, output_notebook\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "import gensim\n",
    "import splitter\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rithika/anaconda3/envs/deep/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ae25cc1fd345>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading googlenews vec\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/rithika/Documents/247ai/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning input data to get predictor values and target\n",
    "def prepare_data(fname):\n",
    "    filename = os.path.join('/Users/rithika/Documents/247ai/datasets', fname)\n",
    "    if os.path.isfile(filename):\n",
    "        dat = pd.read_csv(filename)\n",
    "    else:\n",
    "        print(\"no such file exists at this time\")\n",
    "    dat['Adcopy']=dat['Headline 1']+' '+dat['Headline 2']+' '+dat['Description']\n",
    "    #dat['Adcopy'] = dat['Description']\n",
    "    dat = dat[['Adcopy','Clicks']]\n",
    "    dat = dat[pd.notnull(dat['Adcopy'])]\n",
    "    dat['Adcopy'] = dat['Adcopy'].replace('http\\S+|www.\\S+', '', regex=True)\n",
    "    dat['Adcopy'].replace('[™!®\"#\\'©()*+,-./:;<=>?@\\&[\\]^_`{|}~’”“′‘\\\\\\%0123456789£]',' ',inplace=True,regex=True)\n",
    "    dat['Adcopy'].replace('  ',' ',inplace=True,regex=True)\n",
    "    dat['Adcopy'].replace('   ',' ',inplace=True,regex=True)\n",
    "    dat['Adcopy'].replace('  ',' ',inplace=True,regex=True)\n",
    "    dat['Adcopy'] = dat['Adcopy'].str.lower()\n",
    "    dat['Adcopy'] = dat['Adcopy'].apply(lambda x: ' '.join([wordnet_lemmatizer.lemmatize(str(word)) for word in x.split()]))\n",
    "    dat['Copy'] = dat['Adcopy'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    data = dat[['Copy','Clicks']]\n",
    "    datum = data.groupby('Copy')['Clicks'].mean().reset_index()\n",
    "    print(len(datum))\n",
    "    return datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints words from the dataset which is not present in google news vec(simply for reducing the runtime of preprocessing)\n",
    "def no_present(da, model):\n",
    "    #loading googlenews vec\n",
    "    #model = gensim.models.KeyedVectors.load_word2vec_format('/Users/rithika/Documents/247ai/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    nokey = list()\n",
    "    for sentence in da:\n",
    "        for word in sentence.split(' '): \n",
    "            if word not in model:\n",
    "                if word not in nokey:\n",
    "                    nokey.append(word)\n",
    "    #print(nokey)\n",
    "    #print(len(nokey))\n",
    "    return nokey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits the compound words\n",
    "def spli(row, nokey):\n",
    "    if row in nokey:\n",
    "        y = splitter.split(row) #compound word splitter\n",
    "        if y != row and y != '':\n",
    "            return ' '.join(y)\n",
    "        else:\n",
    "            wo = Word(row).correct() #spellcorrector\n",
    "            return wo\n",
    "    else:\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#furthur cleans the data and returns the input and output values\n",
    "def clean_dat(data,model):\n",
    "    nokey = no_present(data['Copy'],model)\n",
    "    data['Copy'] = data['Copy'].apply(lambda x: ' '.join([spli(str(word),nokey) for word in x.split()]))\n",
    "    datu = data.groupby('Copy')['Clicks'].mean().reset_index()\n",
    "    clean_d = datu['Copy'] #predictor values\n",
    "    res = datu['Clicks'] #target\n",
    "    print(len(clean_d))\n",
    "    print(len(res))\n",
    "    return clean_d, res, datu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes rows which have words from foreign language\n",
    "def remove_lang(clean_d, datu, model):\n",
    "    key_not = no_present(clean_d, model)\n",
    "    ind = []\n",
    "    count = 0\n",
    "    for sen in clean_d:\n",
    "        for word in sen.split(' '):\n",
    "            if word in key_not:\n",
    "                if word not in stop:\n",
    "                    if count not in ind:\n",
    "                        ind.append(count)\n",
    "        count+=1\n",
    "    cl_data = datu.drop(datu.index[ind])  \n",
    "    cl_data['Copy'] = cl_data['Copy'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    #print(ind)\n",
    "    clean_data = cl_data['Copy']\n",
    "    result = cl_data['Clicks']\n",
    "    print(len(clean_data))\n",
    "    print(len(result))\n",
    "    return clean_data, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_tfidf(cleaned_data, result):\n",
    "    cleaned_data = cleaned_data.tolist()\n",
    "    # create the transform\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # tokenize and build vocab\n",
    "    vec_train = vectorizer.fit_transform(cleaned_data)\n",
    "    #Save vectorizer.vocabulary_\n",
    "    pickle.dump(vectorizer,open(\"feature_bow.pkl\",\"wb\")) # used to predict unseen data\n",
    "    # encode document\n",
    "    #vector = vectorizer.transform(cleaned_data)\n",
    "    X = np.array(vec_train.toarray())\n",
    "    y = np.array(result)\n",
    "    # summarize\n",
    "    #print(vectorizer.vocabulary_)\n",
    "    # summarize encoded vector\n",
    "    #print(vec_train.shape)\n",
    "    #print(vec_train.toarray())\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of random float values within a range\n",
    "def sample_floats(low, high, k=1):\n",
    "    result = []\n",
    "    seen = set()\n",
    "    for i in range(k):\n",
    "        x = random.uniform(low, high)\n",
    "        while x in seen:\n",
    "            x = random.uniform(low, high)\n",
    "        seen.add(x)\n",
    "        result.append(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates vector array for words not in googlenews vec\n",
    "def gen_arr():\n",
    "    c = [0] * 300\n",
    "    dup = sample_floats(-0.001, 0.001, 300)\n",
    "    r = np.random.randint(5, 30)\n",
    "    for i in range(r):\n",
    "        ind = np.random.randint(0, len(c) - 1)\n",
    "        c[ind] = np.random.choice(dup)\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the words not present in google model\n",
    "def get_valvec(data):\n",
    "    nokey = list()\n",
    "    my_dict = {}\n",
    "    for sentence in data:\n",
    "        for word in sentence.split(' '): \n",
    "            if word not in model: \n",
    "                if word not in nokey: #for words not present    \n",
    "                    nokey.append(word)\n",
    "                    ne = gen_arr()\n",
    "                    my_dict[word] = ne\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting feature vectors for hello fresh\n",
    "def get_vectors(data):\n",
    "    extra_vec = get_valvec(cleaned_data)     #dictionary of words not present in google with their vecs \n",
    "    in_arr = []\n",
    "    for sentence in data:\n",
    "        n_arr = []\n",
    "        for word in sentence.split(' '): \n",
    "            arr = []\n",
    "            try:\n",
    "                for single in model[word]:\n",
    "                    arr.append(single)\n",
    "            except ValueError:\n",
    "                print(\"Value error\")\n",
    "            except KeyError:\n",
    "                n_w = word.title()\n",
    "                nn_w = Word(word).correct()\n",
    "                if n_w in model:\n",
    "                    try:\n",
    "                        for single in model[n_w]:\n",
    "                            arr.append(single)\n",
    "                    except ValueError:\n",
    "                        print(\"Value error\")\n",
    "                elif nn_w in model:\n",
    "                    try:\n",
    "                        for single in model[nn_w]:\n",
    "                            arr.append(single)\n",
    "                    except ValueError:\n",
    "                        print(\"Value error\")\n",
    "                else:\n",
    "                    v = extra_vec.get(word)\n",
    "                    for i in v:\n",
    "                        arr.append(i)\n",
    "            n_arr.append(arr)\n",
    "            arry = []\n",
    "            arry = np.sum(n_arr,axis=0)\n",
    "        in_arr.append(arry)   \n",
    "    return in_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(cleaned_data, result):\n",
    "    #cleaned_data = cleaned_data.astype(str)\n",
    "    res = get_vectors(cleaned_data)\n",
    "    x = np.array(res).tolist()\n",
    "    X = np.array(x)\n",
    "    y = np.array(result)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_uni_encoder(cleaned_data, result):\n",
    "    cleaned_data = cleaned_data.tolist()\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" \n",
    "    #to suppress INFO prints by tensorflow\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    # Import the Universal Sentence Encoder's TF Hub module\n",
    "    embed = hub.Module(module_url)\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(embed(cleaned_data))\n",
    "    #print(message_embeddings.shape)\n",
    "    X = message_embeddings\n",
    "    y = np.array(result)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_se(cleaned_data, result):    \n",
    "    from models import InferSent\n",
    "    V = 2\n",
    "    MODEL_PATH = '/Users/rithika/Documents/InferSent/infersent%s.pkl' % V\n",
    "    params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                    'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "    infersent = InferSent(params_model)\n",
    "    infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "    W2V_PATH = '/Users/rithika/Documents/InferSent/crawl-300d-2M.vec'\n",
    "    infersent.set_w2v_path(W2V_PATH)\n",
    "    infersent.build_vocab(cleaned_data, tokenize=False)\n",
    "    embeddings = infersent.encode(cleaned_data, tokenize=False) #True)\n",
    "    print('nb sentences encoded : {0}'.format(len(embeddings)))\n",
    "    X = embeddings\n",
    "    y = np.array(result)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running the functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "#normal preprocess\n",
    "data = prepare_data('Subaru_2018-06-25_2018-07-24.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['customise', 'favourite', 'impreza', 'levorg', 'carplay', 'brz', 'cvt', 'imprezas', 'nowarrived', 'wrx']\n"
     ]
    }
   ],
   "source": [
    "#prints words from the dataset which is not present in google news vec\n",
    "key1 = list()\n",
    "for sentence in data['Copy']:\n",
    "    for word in sentence.split(' '): \n",
    "        if word not in model:\n",
    "            if word not in key1:\n",
    "                key1.append(word)\n",
    "                \n",
    "print(len(key1))\n",
    "print(key1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = data['Copy']\n",
    "result = data['Clicks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "#split and correct\n",
    "clean_d, res, datu = clean_dat(data,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['favourite']\n"
     ]
    }
   ],
   "source": [
    "#prints words from the dataset which is not present in google news vec\n",
    "key2 = list()\n",
    "for sentence in clean_d:\n",
    "    for word in sentence.split(' '): \n",
    "        if word not in model:\n",
    "            if word not in key2:\n",
    "                key2.append(word)                \n",
    "print(len(key2))\n",
    "print(key2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = clean_d\n",
    "result = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n",
      "266\n"
     ]
    }
   ],
   "source": [
    "cleaned_data, result = remove_lang(clean_d, datu, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#prints words from the dataset which is not present in google news vec\n",
    "key3 = list()\n",
    "for sentence in cleaned_data:\n",
    "    for word in sentence.split(' '): \n",
    "        if word not in model:\n",
    "            if word not in key3:\n",
    "                key3.append(word)\n",
    "                \n",
    "print(len(key3))\n",
    "print(key3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_Regressor(X, y):\n",
    "    #kfold\n",
    "    kf = model_selection.KFold(n_splits=5) \n",
    "    kf.get_n_splits(X)\n",
    "    #print(kf)\n",
    "    #creating instance of RFRegressor \n",
    "    model1 = RandomForestRegressor(n_estimators=500,max_features='sqrt',n_jobs=-1,min_samples_leaf=60)#cross val split\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #print('TRAIN:', train_index, 'TEST:', test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]    \n",
    "    #training\n",
    "    model1.fit(X_train, y_train)\n",
    "    #evaluating\n",
    "    y_pred = model1.predict(X_test)\n",
    "    #monotonic relationship as the relation between the variables is non linear\n",
    "    spearman = spearmanr(y_test, y_pred)\n",
    "    pearson = pearsonr(y_test, y_pred)\n",
    "    print(f'Test data Spearman correlation: {spearman[0]:.3}')\n",
    "    print(f'Test data pearson correlation: {pearson[0]:.3}')\n",
    "    print('MSE')\n",
    "    print(metrics.mean_squared_error(y_test, y_pred)) #mean square error\n",
    "    print('R2')\n",
    "    print(metrics.r2_score(y_test, y_pred)) #r2 score\n",
    "    print('MAE')\n",
    "    print(metrics.mean_absolute_error(y_test, y_pred)) #mae\n",
    "    print('Variance Score')\n",
    "    print(metrics.explained_variance_score(y_test, y_pred)) #mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, y):\n",
    "    #kfold\n",
    "    kf = model_selection.KFold(n_splits=5) \n",
    "    kf.get_n_splits(X)\n",
    "    #print(kf)\n",
    "    #cross val split with score and r2\n",
    "    for train_index, test_index in kf.split(X):\n",
    "    #print('TRAIN:', train_index, 'TEST:', test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model2 = Ridge(alpha=0.5)\n",
    "    # Fit the model\n",
    "    model2.fit(X_train, y_train)\n",
    "    y_pred = model2.predict(X_test)\n",
    "    \n",
    "    spearman = spearmanr(y_test, y_pred)\n",
    "    pearson = pearsonr(y_test, y_pred)\n",
    "    print(f'Test data Spearman correlation: {spearman[0]:.3}')\n",
    "    print(f'Test data pearson correlation: {pearson[0]:.3}')\n",
    "    print('MSE')\n",
    "    print(metrics.mean_squared_error(y_test, y_pred)) #mean square error\n",
    "    print('R2')\n",
    "    print(metrics.r2_score(y_test, y_pred)) #r2 score\n",
    "    print('MAE')\n",
    "    print(metrics.mean_absolute_error(y_test, y_pred)) #mae\n",
    "    print('Variance Score')\n",
    "    print(metrics.explained_variance_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19118, 7112)\n",
      "(19118,)\n"
     ]
    }
   ],
   "source": [
    "X, y = bow_tfidf(cleaned_data, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Spearman correlation: 0.107\n",
      "Test data pearson correlation: 0.13\n",
      "MSE\n",
      "175.5643438940584\n",
      "R2\n",
      "0.0030529520362929663\n",
      "MAE\n",
      "2.312211835050464\n",
      "Variance Score\n",
      "0.007593508054656528\n"
     ]
    }
   ],
   "source": [
    "RF_Regressor(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Spearman correlation: 0.329\n",
      "Test data pearson correlation: 0.182\n",
      "MSE\n",
      "241.9117751714945\n",
      "R2\n",
      "-0.37370279622617275\n",
      "MAE\n",
      "4.040666373203655\n",
      "Variance Score\n",
      "-0.37358883686906097\n"
     ]
    }
   ],
   "source": [
    "ridge_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19118, 300)\n",
      "(19118,)\n"
     ]
    }
   ],
   "source": [
    "X, y = word2vec(cleaned_data, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Spearman correlation: 0.183\n",
      "Test data pearson correlation: 0.182\n",
      "MSE\n",
      "170.40616252695142\n",
      "R2\n",
      "0.03234382951595838\n",
      "MAE\n",
      "2.69902181241004\n",
      "Variance Score\n",
      "0.032356003257481114\n"
     ]
    }
   ],
   "source": [
    "RF_Regressor(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Spearman correlation: 0.122\n",
      "Test data pearson correlation: 0.189\n",
      "MSE\n",
      "172.43145100928444\n",
      "R2\n",
      "0.02084317209912434\n",
      "MAE\n",
      "3.6121474272703673\n",
      "Variance Score\n",
      "0.025413444333375446\n"
     ]
    }
   ],
   "source": [
    "ridge_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19118, 512)\n",
      "(19118,)\n"
     ]
    }
   ],
   "source": [
    "X, y = g_uni_encoder(cleaned_data, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Spearman correlation: 0.186\n",
      "Test data pearson correlation: 0.136\n",
      "MSE\n",
      "173.01166707070965\n",
      "R2\n",
      "0.017548398930558484\n",
      "MAE\n",
      "3.033783439136066\n",
      "Variance Score\n",
      "0.01844695496655413\n"
     ]
    }
   ],
   "source": [
    "RF_Regressor(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Spearman correlation: 0.227\n",
      "Test data pearson correlation: 0.154\n",
      "MSE\n",
      "174.21428829626694\n",
      "R2\n",
      "0.01071928059112337\n",
      "MAE\n",
      "3.640063386800593\n",
      "Variance Score\n",
      "0.01793112795276053\n"
     ]
    }
   ],
   "source": [
    "ridge_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3033(/7123) words with w2v vectors\n",
      "Vocab size : 3033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rithika/Documents/247ai/summer-internship/models.py:222: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  sentences[stidx:stidx + bsize]), volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sentences encoded : 19118\n",
      "(19118, 4096)\n",
      "(19118,)\n"
     ]
    }
   ],
   "source": [
    "X, y = infer_se(cleaned_data, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Spearman correlation: 0.145\n",
      "Test data pearson correlation: 0.201\n",
      "MSE\n",
      "170.32249493566226\n",
      "R2\n",
      "0.03281893827834059\n",
      "MAE\n",
      "3.3174003577720534\n",
      "Variance Score\n",
      "0.03745087650409262\n"
     ]
    }
   ],
   "source": [
    "RF_Regressor(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Spearman correlation: 0.221\n",
      "Test data pearson correlation: 0.123\n",
      "MSE\n",
      "347.85919004692136\n",
      "R2\n",
      "-0.9753281613582871\n",
      "MAE\n",
      "6.779065207522452\n",
      "Variance Score\n",
      "-0.9703489877051856\n"
     ]
    }
   ],
   "source": [
    "ridge_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
