{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import Imputer \n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from sklearn.externals import joblib\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "from bokeh.io import push_notebook, output_notebook\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "import gensim\n",
    "import splitter\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading googlenews vec\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/rithika/Documents/247ai/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to use tensor flow 1.4.0 because the latest version(1.8.0) is not compatible with keras 2.1.5\n",
    "# dont use conda for tf and k installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/rithika/anaconda3/envs/deep/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.utils.np_utils import np as np\n",
    "from keras.models import Model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning input data to get predictor values and target\n",
    "def prepare_data(fname):\n",
    "    filename = os.path.join('/Users/rithika/Documents/247ai/datasets', fname)\n",
    "    if os.path.isfile(filename):\n",
    "        dat = pd.read_csv(filename)\n",
    "    else:\n",
    "        print(\"no such file exists at this time\")\n",
    "    dat['Adcopy']=dat['Headline 1']+' '+dat['Headline 2']+' '+dat['Description']\n",
    "    #dat['Adcopy'] = dat['Description']\n",
    "    dat = dat[['Adcopy','Clicks']]\n",
    "    dat = dat[pd.notnull(dat['Adcopy'])]\n",
    "    dat['Adcopy'] = dat['Adcopy'].replace('http\\S+|www.\\S+', '', regex=True)\n",
    "    dat['Adcopy'].replace('[™!®\"#\\'©()*+,-./:;<=>?@\\&[\\]^_`{|}~’”“′‘\\\\\\%0123456789£]',' ',inplace=True,regex=True)\n",
    "    dat['Adcopy'].replace('  ',' ',inplace=True,regex=True)\n",
    "    dat['Adcopy'].replace('   ',' ',inplace=True,regex=True)\n",
    "    dat['Adcopy'].replace('  ',' ',inplace=True,regex=True)\n",
    "    dat['Adcopy'] = dat['Adcopy'].str.lower()\n",
    "    dat['Adcopy'] = dat['Adcopy'].apply(lambda x: ' '.join([wordnet_lemmatizer.lemmatize(str(word)) for word in x.split()]))\n",
    "    dat['Copy'] = dat['Adcopy'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    data = dat[['Copy','Clicks']]\n",
    "    datum = data.groupby('Copy')['Clicks'].mean().reset_index()\n",
    "    print(len(datum))\n",
    "    return datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints words from the dataset which is not present in google news vec(simply for reducing the runtime of preprocessing)\n",
    "def no_present(da, model):\n",
    "    #loading googlenews vec\n",
    "    #model = gensim.models.KeyedVectors.load_word2vec_format('/Users/rithika/Documents/247ai/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    nokey = list()\n",
    "    for sentence in da:\n",
    "        for word in sentence.split(' '): \n",
    "            if word not in model:\n",
    "                if word not in nokey:\n",
    "                    nokey.append(word)\n",
    "    #print(nokey)\n",
    "    #print(len(nokey))\n",
    "    return nokey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits the compound words\n",
    "def spli(row, nokey):\n",
    "    if row in nokey:\n",
    "        y = splitter.split(row) #compound word splitter\n",
    "        if y != row and y != '':\n",
    "            return ' '.join(y)\n",
    "        else:\n",
    "            wo = Word(row).correct() #spellcorrector\n",
    "            return wo\n",
    "    else:\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#furthur cleans the data and returns the input and output values\n",
    "def clean_dat(data,model):\n",
    "    nokey = no_present(data['Copy'],model)\n",
    "    data['Copy'] = data['Copy'].apply(lambda x: ' '.join([spli(str(word),nokey) for word in x.split()]))\n",
    "    datu = data.groupby('Copy')['Clicks'].mean().reset_index()\n",
    "    clean_d = datu['Copy'] #predictor values\n",
    "    res = datu['Clicks'] #target\n",
    "    print(len(clean_d))\n",
    "    print(len(res))\n",
    "    return clean_d, res, datu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes rows which have words from foreign language\n",
    "def remove_lang(clean_d, datu, model):\n",
    "    key_not = no_present(clean_d, model)\n",
    "    ind = []\n",
    "    count = 0\n",
    "    for sen in clean_d:\n",
    "        for word in sen.split(' '):\n",
    "            if word in key_not:\n",
    "                if word not in stop:\n",
    "                    if count not in ind:\n",
    "                        ind.append(count)\n",
    "        count+=1\n",
    "    cl_data = datu.drop(datu.index[ind])  \n",
    "    cl_data['Copy'] = cl_data['Copy'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    #print(ind)\n",
    "    clean_data = cl_data['Copy']\n",
    "    result = cl_data['Clicks']\n",
    "    print(len(clean_data))\n",
    "    print(len(result))\n",
    "    return clean_data, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_Regressor(X, y):\n",
    "    #kfold\n",
    "    kf = model_selection.KFold(n_splits=5) \n",
    "    kf.get_n_splits(X)\n",
    "    #print(kf)\n",
    "    #creating instance of RFRegressor \n",
    "    model1 = RandomForestRegressor(n_estimators=500,max_features='sqrt',n_jobs=-1,min_samples_leaf=60)#cross val split\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #print('TRAIN:', train_index, 'TEST:', test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]    \n",
    "    #training\n",
    "    model1.fit(X_train, y_train)\n",
    "    #evaluating\n",
    "    y_pred = model1.predict(X_test)\n",
    "    #monotonic relationship as the relation between the variables is non linear\n",
    "    spearman = spearmanr(y_test, y_pred)\n",
    "    pearson = pearsonr(y_test, y_pred)\n",
    "    print(f'Test data Spearman correlation: {spearman[0]:.3}')\n",
    "    print(f'Test data pearson correlation: {pearson[0]:.3}')\n",
    "    print('MSE')\n",
    "    print(metrics.mean_squared_error(y_test, y_pred)) #mean square error\n",
    "    print('R2')\n",
    "    print(metrics.r2_score(y_test, y_pred)) #r2 score\n",
    "    print('MAE')\n",
    "    print(metrics.mean_absolute_error(y_test, y_pred)) #mae\n",
    "    print('Variance Score')\n",
    "    print(metrics.explained_variance_score(y_test, y_pred)) #mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, y):\n",
    "    #kfold\n",
    "    kf = model_selection.KFold(n_splits=5) \n",
    "    kf.get_n_splits(X)\n",
    "    #print(kf)\n",
    "    #cross val split with score and r2\n",
    "    for train_index, test_index in kf.split(X):\n",
    "    #print('TRAIN:', train_index, 'TEST:', test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model2 = Ridge(alpha=0.5)\n",
    "    # Fit the model\n",
    "    model2.fit(X_train, y_train)\n",
    "    y_pred = model2.predict(X_test)\n",
    "    \n",
    "    spearman = spearmanr(y_test, y_pred)\n",
    "    pearson = pearsonr(y_test, y_pred)\n",
    "    print(f'Test data Spearman correlation: {spearman[0]:.3}')\n",
    "    print(f'Test data pearson correlation: {pearson[0]:.3}')\n",
    "    print('MSE')\n",
    "    print(metrics.mean_squared_error(y_test, y_pred)) #mean square error\n",
    "    print('R2')\n",
    "    print(metrics.r2_score(y_test, y_pred)) #r2 score\n",
    "    print('MAE')\n",
    "    print(metrics.mean_absolute_error(y_test, y_pred)) #mae\n",
    "    print('Variance Score')\n",
    "    print(metrics.explained_variance_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n"
     ]
    }
   ],
   "source": [
    "data = prepare_data('Wargaming_2018-06-25_2018-07-24.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n",
      "['batailles', 'historiques', 'obtenez', 'téléchargez', 'combattez', 'büyük', 'savaşları', 'burada', 'indir', 'gerçek', 'gücü', 'tadın', 'savaşında', 'şeref', 'kazanın', 'hemen', 'savaşmaya', 'başlayın', 'légendaires', 'jouez', 'blindés', 'améliorés', 'engins', 'revisitées', 'réalistes', 'superbe', 'dizaines', 'maîtriser', 'choisissez', 'vraies', 'superbes', 'graphismes', 'devenez', 'centaines', 'véhicules', 'quête', 'ekte', 'utfordring', 'ferdigheter', 'før', 'seier', 'episke', 'tankskamper', 'utrolig', 'grafikk', 'episk', 'lydspor', 'fuelled', 'wwii', 'samle', 'tankskrigføring', 'aldri', 'kamper', 'hundrevis', 'spille', 'krigføring', 'taktikken', 'ditt', 'gratisspillet', 'trenger', 'forbedrede', 'krigsmaskiner', 'gjenskapte', 'oppdatering', 'savaşı', 'edin', 'hiç', 'görmediğiniz', 'şekilde', 'yüzlerce', 'tankla', 'kalitesinde', 'savaşın', 'dusinvis', 'mestre', 'tankmen', 'gratuits', 'luttez', 'gratuitement', 'prenez', 'perdez', 'fantasyspill', 'begynn', 'kampen', 'historiske', 'oppdag', 'historiebøkene', 'kommandere', 'partant', 'combattre', 'по', 'хардкору', 'сражайся', 'играй', 'картах', 'новой', 'графикой', 'под', 'новый', 'саундтрек', 'ralliez', 'vrais', 'bravez', 'rejoignez', 'commencez', 'assaut', 'défiez', 'krigføringsspill', 'massiv', 'oppgradere', 'tankskamp', 'forbedret', 'nyoppdatering', 'tarihin', 'efsane', 'tankları', 'oyna', 'gelişmiş', 'savaş', 'araçları', 'yenilenen', 'haritalar', 'yeni', 'güncellemede', 'cherchez', 'utilisez', 'remportez', 'épiques', 'smak', 'rå', 'makt', 'lyd', 'бесплатная', 'танковая', 'игра', 'танков', 'миллионов', 'игроков', 'уже', 'играют', 'присоединяйся', 'к', 'боевому', 'братству', 'более', 'карт', 'качестве', 'сочная', 'графика', 'оркестровый', 'высокая', 'детализация', 'красиво', 'качай', 'звуки', 'пражского', 'симфонического', 'оркестра', 'про', 'танки', 'онлайн', 'бесплатно', 'танчики', 'обойма', 'экшена', 'скачай', 'вступай', 'бой', 'новому', 'оцени', 'новую', 'графику', 'музыку', 'окунись', 'танковые', 'сражения', 'получи', 'яркие', 'впечатления', 'пройди', 'регистрацию', 'рулит', 'посмотри', 'также', 'новые', 'карты', '—', 'послушай', 'стань', 'танковым', 'асом', '«танки»', 'просто', 'сыграй', 'геймлей', 'который', 'любят', 'миллионы', 'новая', 'ангар', 'душой', 'это', 'же', 'еще', 'музыка', 'тот', 'самый', 'геймплей', 'современная', 'модели', 'пк', 'танкистов', 'здесь', 'принимай', 'вызов', 'военная', 'стальных', 'машин', 'регистрируйся', 'время', 'коллекция', 'из', 'реалистичные', 'локации', 'второй', 'мировой', 'поле', 'боя', 'зовет', 'скачать', 'те', 'самые', 'их', 'больше', 'отточи', 'военные', 'навыки', 'бою', 'выбирай', 'правильные', 'tanchiki', 'заходи', 'игру', 'нагибай', 'машинах', 'гуглишь', 'ставь', 'многомиллионной', 'армии', 'tanki', 'свыше', 'локаций', 'начать', 'есть', 'только', 'одни', 'любимая', 'во', 'всем', 'мире', 'нас', 'ждем', 'тебя', 'строю', 'не', 'нами', 'млн', 'танчиках', 'пора', 'исследуй', 'целую', 'вселенную', 'легендарных', 'скачал', 'сейчас', 'самое', 'обкатай', 'сражении', 'боевые', 'машины', 'разных', 'наций', 'загрузи', 'командуй', 'танком', 'ждут', 'моделей', 'бронетехники', 'комплексная', 'тактика', 'стратегия', 'дави', 'врага', 'зови', 'друзей', 'взвод', 'новейшая', 'взрывные', 'визуальные', 'эффекты', 'победу', 'апгрейд', 'бои', 'без', 'компромиссов', 'новым', 'саундтреком', 'историческая', 'ищешь', 'ищи', 'героев', 'одной', 'игре', 'стратегию', 'пришло', 'компьютерная', 'прошлого', 'века', 'стать', 'частью', 'танковой', 'вселенной', 'весь', 'мир', 'легенда', 'которую', 'знает', 'легендарная', 'легенд', 'войны', 'ряды', 'будет', 'жарко', 'лидер', 'почувствуй', 'красоту', 'эпичных', 'танковых', 'баталий', 'любишь', 'стратегии', 'команда', 'жар', 'сражений', 'очень', 'командный', 'экшен', 'решай', 'исход', 'непредсказуемо', 'побеждай', 'быстро', 'новых', 'перепробовал', 'все', 'что', 'насчет', 'коллекцию', '«танках»', 'команды', 'многомиллионному', 'арсенал', 'стали', 'жарче', 'музыкой', 'реальные', 'лет', 'отжига', 'катит', 'будущее', 'картам', 'легендарные', 'прокачайся', 'до', 'аса', 'задай', 'жару', 'ударную', 'порцию', 'адреналина', 'танке', 'приобщись', 'легендарный', 'дополняют', 'эпический', 'жаркие', 'та', 'самая', 'играть', 'картинка', 'невероятного', 'качества']\n"
     ]
    }
   ],
   "source": [
    "key1 = list()\n",
    "for sentence in data['Copy']:\n",
    "    for word in sentence.split(' '): \n",
    "        if word not in model:\n",
    "            if word not in key1:\n",
    "                key1.append(word)\n",
    "print(len(key1))\n",
    "print(key1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = data['Copy']\n",
    "result = data['Clicks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "362\n"
     ]
    }
   ],
   "source": [
    "#split and correct\n",
    "clean_d, res, datu = clean_dat(data,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n",
      "['arshin', 'voyna', 'gelişmiş', 'araçları', 'güncellemede', 'téléchargez', 'büyük', 'savaşları', 'a', 'gücü', 'savaşında', 'savaşmaya', 'başlayın', 'légendaires', 'améliorés', 'revisitées', 'réalistes', 'maîtriser', 'choisissez', 'devriez', 'utfordring', 'sevier', 'fiske', 'utrolig', 'grafikk', 'lydspor', 'tankskrigføring', 'kammer', 'krigsmaskiner', 'gjenskapte', 'krigføring', 'taktikken', 'görmediğiniz', 'şekilde', 'yüzlerce', 'savaşın', 'gratuitement', 'of', 'самая', 'онлайн', 'игра', 'время', 'играть', 'более', 'танков', 'современная', 'графика', 'реалистичные', 'карты', 'бой', 'to', 'historiebøkene', 'хардкору', 'сражайся', 'играй', 'картах', 'новой', 'графикой', 'под', 'новый', 'саундтрек', 'krigføringsspill', 'nyoppdatering', 'utilised', 'piquet', 'новому', 'оцени', 'новую', 'графику', 'музыку', 'окунись', 'танковые', 'сражения', 'получи', 'яркие', 'впечатления', 'бесплатная', 'танковая', 'миллионов', 'игроков', 'уже', 'играют', 'присоединяйся', 'к', 'боевому', 'братству', 'карт', 'качестве', 'сочная', 'оркестровый', 'высокая', 'детализация', 'красиво', 'качай', 'звуки', 'пражского', 'симфонического', 'оркестра', 'про', 'танки', 'бесплатно', 'танчики', 'обойма', 'экшена', 'скачай', 'вступай', 'пройди', 'регистрацию', 'рулит', 'посмотри', 'также', 'новые', '—', 'послушай', 'стань', 'танковым', 'асом', '«танки»', 'просто', 'сыграй', 'геймлей', 'который', 'любят', 'миллионы', 'новая', 'ангар', 'душой', 'это', 'еще', 'музыка', 'тот', 'самый', 'геймплей', 'модели', 'танкистов', 'здесь', 'принимай', 'вызов', 'военная', 'стальных', 'машин', 'регистрируйся', 'коллекция', 'локации', 'второй', 'мировой', 'поле', 'боя', 'зовет', 'скачать', 'самые', 'больше', 'отточи', 'военные', 'навыки', 'бою', 'выбирай', 'правильные', 'заходи', 'игру', 'нагибай', 'машинах', 'гуглишь', 'ставь', 'многомиллионной', 'армии', 'свыше', 'локаций', 'начать', 'есть', 'только', 'одни', 'любимая', 'всем', 'мире', 'нас', 'ждем', 'тебя', 'строю', 'нами', 'млн', 'танчиках', 'пора', 'исследуй', 'целую', 'вселенную', 'легендарных', 'скачал', 'сейчас', 'самое', 'обкатай', 'сражении', 'боевые', 'машины', 'разных', 'наций', 'загрузи', 'командуй', 'танком', 'ждут', 'моделей', 'бронетехники', 'комплексная', 'тактика', 'стратегия', 'дави', 'врага', 'зови', 'друзей', 'взвод', 'новейшая', 'взрывные', 'визуальные', 'эффекты', 'победу', 'апгрейд', 'бои', 'без', 'компромиссов', 'новым', 'саундтреком', 'историческая', 'ищешь', 'ищи', 'героев', 'одной', 'игре', 'стратегию', 'пришло', 'компьютерная', 'прошлого', 'века', 'стать', 'частью', 'танковой', 'вселенной', 'весь', 'мир', 'легенда', 'которую', 'знает', 'легендарная', 'легенд', 'войны', 'ряды', 'будет', 'жарко', 'лидер', 'почувствуй', 'красоту', 'эпичных', 'танковых', 'баталий', 'любишь', 'стратегии', 'команда', 'жар', 'сражений', 'очень', 'командный', 'экшен', 'решай', 'исход', 'непредсказуемо', 'побеждай', 'быстро', 'новых', 'перепробовал', 'все', 'что', 'насчет', 'коллекцию', '«танках»', 'команды', 'многомиллионному', 'арсенал', 'стали', 'жарче', 'музыкой', 'реальные', 'лет', 'отжига', 'катит', 'будущее', 'картам', 'легендарные', 'прокачайся', 'аса', 'задай', 'жару', 'ударную', 'порцию', 'адреналина', 'танке', 'приобщись', 'легендарный', 'дополняют', 'эпический', 'жаркие', 'картинка', 'невероятного', 'качества']\n"
     ]
    }
   ],
   "source": [
    "key2 = list()\n",
    "for sentence in clean_d:\n",
    "    for word in sentence.split(' '): \n",
    "        if word not in model:\n",
    "            if word not in key2:\n",
    "                key2.append(word)\n",
    "print(len(key2))\n",
    "print(key2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = clean_d\n",
    "result = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n",
      "266\n"
     ]
    }
   ],
   "source": [
    "cleaned_data, result = remove_lang(clean_d, datu, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "key3 = list()\n",
    "for sentence in cleaned_data:\n",
    "    for word in sentence.split(' '): \n",
    "        if word not in model:\n",
    "            if word not in key3:\n",
    "                key3.append(word)\n",
    "print(len(key3))\n",
    "print(key3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data.tolist() #predictor values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess for keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=2000\n",
    "maxlen=20\n",
    "tokenizer = Tokenizer(num_words = num_words, split=' ')\n",
    "tokenizer.fit_on_texts(cleaned_data)\n",
    "seqs = tokenizer.texts_to_sequences(cleaned_data)\n",
    "pad_seqs = []\n",
    "for i in seqs:\n",
    "    if len(i)>4:\n",
    "        pad_seqs.append(i)\n",
    "pad_seqs = pad_sequences(pad_seqs,maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,  45, ...,  20,   9,   8],\n",
       "       [  0,   0,  45, ...,  13,  26,  29],\n",
       "       [  0,  45,   2, ...,  20,   9,   8],\n",
       "       ..., \n",
       "       [  0,   0,   0, ...,  10,  11,  33],\n",
       "       [  0,   0,   0, ...,  10,   9,   8],\n",
       "       [  0,   0,   0, ..., 225,   1,  41]], dtype=int32)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266, 20)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [00:00<00:00, 90568.97it/s]\n"
     ]
    }
   ],
   "source": [
    "x_skip = []\n",
    "y_before = []\n",
    "y_after = []\n",
    "for i in tqdm(range(1,len(seqs)-1)):\n",
    "    if len(seqs[i])>4:\n",
    "        x_skip.append(pad_seqs[i].tolist())\n",
    "        y_before.append(pad_seqs[i-1].tolist())\n",
    "        y_after.append(pad_seqs[i+1].tolist())\n",
    "x_before = np.matrix([[0]+i[:-1] for i in y_before])\n",
    "x_after = np.matrix([[0]+i[:-1] for i in y_after])\n",
    "x_skip = np.matrix(x_skip)\n",
    "y_before = np.matrix(y_before)\n",
    "y_after = np.matrix(y_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264, 20)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_skip.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264, 20)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_before.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264, 20)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_after.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq architecture to extract sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Dec-In-before (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Enc-In (InputLayer)             (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Dec-In-after (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Embedding)           (None, 20, 150)      300000      Dec-In-before[0][0]              \n",
      "                                                                 Dec-In-after[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Enc-Model (Model)               (None, 128)          407136      Enc-In[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Dec-GRU-before (GRU)            [(None, 20, 128), (N 107136      Embedding[1][0]                  \n",
      "                                                                 Enc-Model[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Dec-GRU-after (GRU)             [(None, 20, 128), (N 107136      Embedding[2][0]                  \n",
      "                                                                 Enc-Model[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Final-Out-Dense-before (Dense)  (None, 20, 2000)     258000      Dec-GRU-before[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Final-Out-Dense-after (Dense)   (None, 20, 2000)     258000      Dec-GRU-after[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,137,408\n",
      "Trainable params: 1,137,408\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 232 samples, validate on 32 samples\n",
      "Epoch 1/10\n",
      "232/232 [==============================] - 8s 32ms/step - loss: 15.1802 - Final-Out-Dense-before_loss: 7.5909 - Final-Out-Dense-after_loss: 7.5892 - val_loss: 15.1249 - val_Final-Out-Dense-before_loss: 7.5645 - val_Final-Out-Dense-after_loss: 7.5604\n",
      "Epoch 2/10\n",
      "232/232 [==============================] - 4s 17ms/step - loss: 15.0920 - Final-Out-Dense-before_loss: 7.5488 - Final-Out-Dense-after_loss: 7.5432 - val_loss: 14.8960 - val_Final-Out-Dense-before_loss: 7.4561 - val_Final-Out-Dense-after_loss: 7.4399\n",
      "Epoch 3/10\n",
      "232/232 [==============================] - 4s 17ms/step - loss: 14.5915 - Final-Out-Dense-before_loss: 7.3173 - Final-Out-Dense-after_loss: 7.2743 - val_loss: 11.3656 - val_Final-Out-Dense-before_loss: 5.7907 - val_Final-Out-Dense-after_loss: 5.5749\n",
      "Epoch 4/10\n",
      "232/232 [==============================] - 4s 17ms/step - loss: 9.8701 - Final-Out-Dense-before_loss: 5.0007 - Final-Out-Dense-after_loss: 4.8694 - val_loss: 7.8843 - val_Final-Out-Dense-before_loss: 3.9852 - val_Final-Out-Dense-after_loss: 3.8992\n",
      "Epoch 5/10\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 7.9457 - Final-Out-Dense-before_loss: 3.9951 - Final-Out-Dense-after_loss: 3.9506 - val_loss: 7.1634 - val_Final-Out-Dense-before_loss: 3.6022 - val_Final-Out-Dense-after_loss: 3.5612\n",
      "Epoch 6/10\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 7.4353 - Final-Out-Dense-before_loss: 3.7293 - Final-Out-Dense-after_loss: 3.7060 - val_loss: 6.8987 - val_Final-Out-Dense-before_loss: 3.4609 - val_Final-Out-Dense-after_loss: 3.4378\n",
      "Epoch 7/10\n",
      "232/232 [==============================] - 4s 17ms/step - loss: 7.2314 - Final-Out-Dense-before_loss: 3.6270 - Final-Out-Dense-after_loss: 3.6044 - val_loss: 6.7598 - val_Final-Out-Dense-before_loss: 3.3898 - val_Final-Out-Dense-after_loss: 3.3700\n",
      "Epoch 8/10\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 7.1212 - Final-Out-Dense-before_loss: 3.5730 - Final-Out-Dense-after_loss: 3.5482 - val_loss: 6.6778 - val_Final-Out-Dense-before_loss: 3.3389 - val_Final-Out-Dense-after_loss: 3.3389\n",
      "Epoch 9/10\n",
      "232/232 [==============================] - 4s 17ms/step - loss: 7.0397 - Final-Out-Dense-before_loss: 3.5300 - Final-Out-Dense-after_loss: 3.5097 - val_loss: 6.5971 - val_Final-Out-Dense-before_loss: 3.2914 - val_Final-Out-Dense-after_loss: 3.3057\n",
      "Epoch 10/10\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 6.9767 - Final-Out-Dense-before_loss: 3.4982 - Final-Out-Dense-after_loss: 3.4784 - val_loss: 6.5387 - val_Final-Out-Dense-before_loss: 3.2622 - val_Final-Out-Dense-after_loss: 3.2765\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 150\n",
    "latent_dim = 128\n",
    "batch_size = 64\n",
    "# Encoder Model \n",
    "encoder_inputs = Input(shape=(maxlen,), name='Enc-In')\n",
    "emb_layer = Embedding(num_words, embed_dim,input_length = maxlen, name='Embedding', mask_zero=False)\n",
    "x = emb_layer(encoder_inputs)\n",
    "_, state_h = GRU(latent_dim, return_state=True, name='Enc-Last-GRU')(x)\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Enc-Model')\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "# Decoder Model \n",
    "decoder_inputs_before = Input(shape=(None,), name='Dec-In-before')  # for teacher forcing\n",
    "dec_emb_before = emb_layer(decoder_inputs_before)\n",
    "decoder_gru_before = GRU(latent_dim, return_state=True, return_sequences=True, name='Dec-GRU-before')\n",
    "decoder_gru_output_before, _ = decoder_gru_before(dec_emb_before, initial_state=seq2seq_encoder_out)\n",
    "decoder_dense_before = Dense(num_words, activation='softmax', name='Final-Out-Dense-before')\n",
    "decoder_outputs_before = decoder_dense_before(decoder_gru_output_before)\n",
    "decoder_inputs_after = Input(shape=(None,), name='Dec-In-after')  # for teacher forcing\n",
    "dec_emb_after = emb_layer(decoder_inputs_after)\n",
    "decoder_gru_after = GRU(latent_dim, return_state=True, return_sequences=True, name='Dec-GRU-after')\n",
    "decoder_gru_output_after, _ = decoder_gru_after(dec_emb_after, initial_state=seq2seq_encoder_out)\n",
    "decoder_dense_after = Dense(num_words, activation='softmax', name='Final-Out-Dense-after')\n",
    "decoder_outputs_after = decoder_dense_after(decoder_gru_output_after)\n",
    "# Seq2Seq Model \n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs_before,decoder_inputs_after], [decoder_outputs_before,decoder_outputs_after])\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
    "seq2seq_Model.summary()\n",
    "history = seq2seq_Model.fit([x_skip,x_before, x_after], [np.expand_dims(y_before, -1),np.expand_dims(y_after, -1)],\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_split=0.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_rep = tokenizer.texts_to_sequences(cleaned_data)\n",
    "sen_rep = pad_sequences(sen_rep,maxlen=maxlen)\n",
    "x = encoder_model.predict(sen_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Spearman correlation: -0.0293\n",
      "Test data pearson correlation: 0.00497\n",
      "MSE\n",
      "188.082230858\n",
      "R2\n",
      "-0.0225713641569\n",
      "MAE\n",
      "8.27915081639\n",
      "Variance Score\n",
      "-0.00789840212195\n"
     ]
    }
   ],
   "source": [
    "RF_Regressor(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Spearman correlation: -0.13\n",
      "Test data pearson correlation: -0.0162\n",
      "MSE\n",
      "186.976289043\n",
      "R2\n",
      "-0.0165585450529\n",
      "MAE\n",
      "8.30819606191\n",
      "Variance Score\n",
      "-0.00211053468485\n"
     ]
    }
   ],
   "source": [
    "ridge_regression(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.66155065 -1.26285795 -0.76557002 -0.33289663 -0.77377529]\n"
     ]
    }
   ],
   "source": [
    "#cross val score\n",
    "#scores = model_selection.cross_val_score(model1, x, y, cv = 5, scoring ='neg_mean_squared_error')\n",
    "#print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data = ['get instant decision']\n",
    "unseen_data1 = ['decision get instant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract feature \n",
    "sen_rep1 = tokenizer.texts_to_sequences(unseen_data)\n",
    "sen_rep1 = pad_sequences(sen_rep1,maxlen=maxlen)\n",
    "new_v = encoder_model.predict(sen_rep1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model1.predict(new_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12.13641402192663]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract feature \n",
    "sen_rep2 = tokenizer.texts_to_sequences(unseen_data1)\n",
    "sen_rep2 = pad_sequences(sen_rep2,maxlen=maxlen)\n",
    "new_v1 = encoder_model.predict(sen_rep2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model1.predict(new_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.699305266855843]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
